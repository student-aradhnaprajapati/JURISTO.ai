# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u2nB9ntx_UNGIHfPj8J3Nzd32ePAKQx_
"""

import json

from langchain.docstore.document import Document

with open("women_dataset.json", "r", encoding="utf-8") as f:
    data = json.load(f)

def flatten_json(obj, parent_key=""):
    items = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else k
            items.extend(flatten_json(v, new_key))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            new_key = f"{parent_key}[{i}]"
            items.extend(flatten_json(v, new_key))
    else:
        items.append(f"{parent_key}: {obj}")
    return items

flat_data = flatten_json(data)

documents = [Document(page_content=chunk) for chunk in flat_data]

print(f"Flattened into {len(documents)} entries")
print(documents[:3])

from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,     # max size of each chunk
    chunk_overlap=100   # keep overlap for context
)

docs = splitter.split_documents(documents)

print(f"After splitting: {len(docs)} chunks")
print(docs[0].page_content)

!pip install langchain openai faiss-cpu sentence-transformers fastapi uvicorn

!pip install -U langchain-community

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

# !pip install langchain-openai

# !pip install -U langchain-community

import json
import pickle
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load dataset
with open("women_dataset.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Flatten JSON
def flatten_json(obj, parent_key=""):
    items = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            new_key = f"{parent_key}.{k}" if parent_key else k
            items.extend(flatten_json(v, new_key))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            new_key = f"{parent_key}[{i}]"
            items.extend(flatten_json(v, new_key))
    else:
        items.append(f"{parent_key}: {obj}")
    return items

flat_data = flatten_json(data)
documents = [Document(page_content=chunk) for chunk in flat_data]

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
docs = splitter.split_documents(documents)

# Save for reuse
with open("ncw_chunks.pkl", "wb") as f:
    pickle.dump(docs, f)

print(f"Preprocessing complete â€” {len(docs)} chunks saved as ncw_chunks.pkl")

# !pip install sentence-transformers faiss-cpu

import pickle
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

# Load preprocessed chunks (docs from ncw_chunks.pkl)
with open("ncw_chunks.pkl", "rb") as f:
    docs = pickle.load(f)

# Use a local embedding model (MiniLM is lightweight & fast)
model = SentenceTransformer("all-MiniLM-L6-v2")

# Convert chunks to embeddings
texts = [doc.page_content for doc in docs]
embeddings = model.encode(texts, convert_to_numpy=True)

print("Created embeddings:", embeddings.shape)

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print("FAISS index created with", index.ntotal, "vectors")

faiss.write_index(index, "ncw_index.faiss")
with open("ncw_texts.pkl", "wb") as f:
    pickle.dump(texts, f)

# Load FAISS + texts
index = faiss.read_index("ncw_index.faiss")
with open("ncw_texts.pkl", "rb") as f:
    texts = pickle.load(f)

def search(query, top_k=3):
    q_emb = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(q_emb, top_k)
    results = [texts[i] for i in indices[0]]
    return results

# Test retrieval
query = "What is the NCW helpline number?"
print("Query:", query)
print(" Retrieved Chunks:\n", search(query))

query = "How can I file a complaint?"
chunks = search(query, top_k=2)

print("Answer:")
for c in chunks:
    print("-", c)

def chatbot():
    print("NCW Chatbot: ")
    while True:
        query = input("You: ")
        if query.lower() in ["exit", "quit"]:
            print(" Goodbye!")
            break

        results = search(query, top_k=2)

        print("\n: Here:")
        for r in results:
            print("-", r)
        print("\n")

chatbot()